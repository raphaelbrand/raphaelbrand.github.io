<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Raphael Brand | Blog</title><link>https://raphaelbrand.github.io/</link><description></description><lastBuildDate>Sun, 08 Jan 2017 14:30:00 +0100</lastBuildDate><item><title>Local data exploration with the ELK stack</title><link>https://raphaelbrand.github.io/local-data-exploration-with-the-elk-stack.html</link><description>&lt;p&gt;Modern laptops are pretty powerful these days, which makes it possible to analyze medium sized data sets locally instead of using a dedicated server or even a whole cluster.
Popular tools for these tasks are &lt;a href="https://www.r-project.org/"&gt;R&lt;/a&gt; and &lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;, often in combination with the great &lt;a href="https://jupyter.org/"&gt;jupyter notebook&lt;/a&gt;. For this post i am going to show how to do some data analysis with the &lt;a href="https://www.elastic.co/products"&gt;ELK&lt;/a&gt; stack on your local laptop. While this stack is usually used for real-time analytics of time series data, it proves also to be useful for a one time local data analysis. These are the pros that i personally like the most:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very easy to get started.&lt;/li&gt;
&lt;li&gt;Schemaless data structures with elastic search makes it possible to insert documents of different structure into the data base.&lt;/li&gt;
&lt;li&gt;No need to write code to produce charts. We just need to create a logstash file, which describes on how to load the data. Data exploration can be done in the web interface.&lt;/li&gt;
&lt;li&gt;The possibility to scale up easily by inserting the data into an elastic search cluster.&lt;/li&gt;
&lt;li&gt;A lot of charts out of the box, including a world map for geo points.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The data&lt;/h2&gt;
&lt;p&gt;I will use the &lt;a href="https://www.kaggle.com/foenix/slc-crime"&gt;Salt Lake City Crime Report&lt;/a&gt; for 2016. It is in the csv format and has 56244 entries. Each entry is one crime report. Here is an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;,case ,ncic code,description,ibr,occurred,reported,day of week,location,city,city council,police zone,police grid,x_coordinate,y_coordinate,x_gps_coords,y_gps_coords

0,SL201624,2499-13,STOLEN VEHICLE,240,01/01/2016 12:36:15 AM,01/01/2016 12:36:15 AM,6,1400 S EMERY ST,SALT LAKE CITY,2,Z2,123,1882550.0,876155.0,-111.92456865049556,40.73791707496032
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Scope&lt;/h2&gt;
&lt;p&gt;I will not go deeper into how the ELK stack works. A lot of configuration options are left unexplained, since i feel they are not important for experimental usage.&lt;/p&gt;
&lt;h1&gt;Installation&lt;/h1&gt;
&lt;p&gt;Installing is as easy as downloading the archives from the elastic website and extracting them. Also you need to have java8 installed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.zip
wget https://artifacts.elastic.co/downloads/kibana/kibana-5.1.1-linux-x86_64.tar.gz
wget https://artifacts.elastic.co/downloads/logstash/logstash-5.1.1.zip
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Preprocessing&lt;/h1&gt;
&lt;p&gt;After downloading the data we need to preprocess it a bit and insert it into elastic search, which will be done through logstash. While logstash is able to read csv data, in my experience it is easier to feed json data to logstash, sine it needs zero configuration later. Well will use the &lt;a href="http://csvkit.readthedocs.io/en/0.9.1/scripts/csvjson.html"&gt;csvkit&lt;/a&gt; for this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install csvkit
cat SLC_Police_Cases_2016_cleaned_geocoded.csv &lt;span class="p"&gt;|&lt;/span&gt; csvjson --stream &amp;gt; out.jl
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Logstash configuration&lt;/h2&gt;
&lt;p&gt;This is the logstash config i used. Read the comments to understand what each lines does:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;input &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;# Read data from stdin.&lt;/span&gt;
  stdin &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

filter &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;# Parse the json input data.&lt;/span&gt;
  json &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="nb"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; message &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="c1"&gt;# Parse the occurred field into a proper data object.&lt;/span&gt;
  date &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="nv"&gt;match&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;occurred&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;MM/dd/yyyy hh:mm:ss aa&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  mutate &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;#Coordinates need to be a number not a string in ES.&lt;/span&gt;
    &lt;span class="nv"&gt;convert&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;x_gps_coords&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;float&amp;quot;&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;y_gps_coords&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;float&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  mutate &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;# A little hack so we do not have define our own mapping for geopoints.&lt;/span&gt;
    &lt;span class="c1"&gt;# The default logstash mapping defines geoip.location.lon and lat as&lt;/span&gt;
    &lt;span class="c1"&gt;# geopoints for mapping ip addresses to coordinates.&lt;/span&gt;
    &lt;span class="nv"&gt;rename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;x_gps_coords&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;[geoip][location][lon]&amp;quot;&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;y_gps_coords&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;[geoip][location][lat]&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

output &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;# Output data to ES&lt;/span&gt;
  elasticsearch &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="nv"&gt;hosts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;localhost:9200&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
    &lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;logstash-%{+YYYY.MM}&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Loading the data&lt;/h2&gt;
&lt;p&gt;First we need to start ES.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./elasticsearch-5.1.1/bin/elasticsearch
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we can start loading the data. Note that i specify to only use two workers with -w 2, so that ES still has two CPUs for indexing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat out.jl &lt;span class="p"&gt;|&lt;/span&gt;  ./logstash-5.1.1/bin/logstash -f logstash.conf -w 2
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Exploring the data&lt;/h1&gt;
&lt;p&gt;Since we indexed all the data we can now start up kibana and explore the data set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./kibana-5.1.1-linux-x86_64/bin/kibana
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now open http://localhost:5601 in your favorite browser.&lt;/p&gt;
&lt;h2&gt;Disover&lt;/h2&gt;
&lt;p&gt;Make sure to specify the whole year of 2016 in to top right as the time frame first, otherwise you will see nothing. The &lt;em&gt;Discover view&lt;/em&gt; is a good starting point to explore your data for the first time. On the top you will see a graph like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="disover view" src="https://raphaelbrand.github.io/images/elk/discover.png" /&gt;&lt;/p&gt;
&lt;p&gt;It shows the time distribution of our data. Since we used the occurred field for indexing you can see the distribution of the crime reports over the whole year. So what is the spike at the right side of the graph about? We can select only those documents by clicking on this spike. By expanding and inspecting the documents we realize, that these documents have no occured data attached to them and logstash uses by default the date of the insertion.&lt;/p&gt;
&lt;p&gt;On the left side of the view you will see all the fields that were found in all the documents. By clicking on them you will get a quick distribution of values. For example clicking on description gives us:&lt;/p&gt;
&lt;p&gt;&lt;img alt="quick count" src="https://raphaelbrand.github.io/images/elk/quickcount.png" /&gt;&lt;/p&gt;
&lt;p&gt;There is also the possibility to use the search box on the top to filter for specific documents. For example we could filter out documents without a proper occurred date. More about that can be found &lt;a href="https://www.elastic.co/guide/en/beats/packetbeat/current/kibana-queries-filters.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Visualize&lt;/h2&gt;
&lt;p&gt;Finally we can start with the most interesting part, the visualizations. I will not create a step by step tutorial on how to create those, since the interface is pretty easy to figure out. I just want to show some interesting findings. Please be aware that this is not a proper statistical analysis! At all. We don't know much about how the data was generated in the first place. For example if we see more DUIs on Saturday this doesn't mean that more people are driving drunk on Saturday. Could also just be that there are more police men on the street during this day and puling over more drivers.&lt;/p&gt;
&lt;h3&gt;Most dangerous part of the city.&lt;/h3&gt;
&lt;p&gt;By using the map visualization and the coordinates we can create a heat map of the city, to show in which part of the city the most crimes are reported:&lt;/p&gt;
&lt;p&gt;&lt;img alt="quick count" src="https://raphaelbrand.github.io/images/elk/heatmap.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Crimes per time of day.&lt;/h3&gt;
&lt;p&gt;Most crimes are reported on a Saturday.&lt;/p&gt;
&lt;p&gt;&lt;img alt="quick count" src="https://raphaelbrand.github.io/images/elk/dayofweek.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Development of crime rates&lt;/h3&gt;
&lt;p&gt;Let's see if some crimes got more / less frequent over the year.&lt;/p&gt;
&lt;p&gt;&lt;img alt="quick count" src="https://raphaelbrand.github.io/images/elk/crimeratesdevelopment.png" /&gt;&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The ELK stack is a nice alternative to notebooks, for doing some quick exploration of a new data set. It requires little programming and comes with a bunch of useful visualizations out of the box.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raphael Brand</dc:creator><pubDate>Sun, 08 Jan 2017 14:30:00 +0100</pubDate><guid isPermaLink="false">tag:raphaelbrand.github.io,2017-01-08:local-data-exploration-with-the-elk-stack.html</guid><category>elk visualization</category></item><item><title>Run hadoop streaming jobs with pypy on EMR</title><link>https://raphaelbrand.github.io/run-hadoop-streaming-jobs-with-pypy-on-emr.html</link><description>&lt;p&gt;While python is very nice language, it tends to be a lot slower for CPU bound tasks than compiled languages or languages with a &lt;a href="https://en.wikipedia.org/wiki/Just-in-time_compilation"&gt;JIT&lt;/a&gt; compiler &lt;a href="http://benchmarksgame.alioth.debian.org/"&gt;[1]&lt;/a&gt;. While most people just use the standard CPython implementation, there are also alternatives out there. One of them being &lt;a href="http://pypy.org"&gt;pypy&lt;/a&gt;, which has a JIT compiler. The great thing about this is, that for some cases you get some big speedups without changing any thing in your code! Let's try it out with a simple example. We are going to load a file with 5 million lines where each line is a json object. The code looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;fileinput&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;fileinput&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And the results are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPython: 50s&lt;/li&gt;
&lt;li&gt;pypy: 9s&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes you could also use a library like &lt;a href="https://pypi.python.org/pypi/ujson"&gt;ujson&lt;/a&gt; to speed up json parsing, but this example is only to demonstrate how your code can run faster without having to change anything. But be aware that not all libraries are compatible with pypy. For more information read &lt;a href="http://pypy.org/compat.html"&gt;here&lt;/a&gt;. Also pypy usually uses more memory than CPython.&lt;/p&gt;
&lt;h2&gt;Installing pypy on EMR&lt;/h2&gt;
&lt;p&gt;The easiest way to get pypy running on the EMR AMIs is to use the binaries by &lt;a href="https://github.com/squeaky-pl/portable-pypy"&gt;squeaky-pl&lt;/a&gt;. Since all your machines in the cluster have to pull this binary, it is best to put it in a s3 bucket first. Also put this &lt;a href="https://bootstrap.pypa.io/get-pip.py"&gt;file&lt;/a&gt; in the bucket. Then add this to your bootstrap:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;aws s3 cp s3://YOUR-BUCKET/pypy-5.4.1-linux_x86_64-portable.tar.bz2 .
mv pypy-5.4.1-linux_x86_64-portable.tar.bz2 /home/hadoop/
&lt;span class="nb"&gt;cd&lt;/span&gt; /home/hadoop/
tar -xjf pypy-5.4.1-linux_x86_64-portable.tar.bz2
sudo ln -sf /home/hadoop/pypy-5.4.1-linux_x86_64-portable.tar.bz2/bin/pypy /usr/bin/pypy
aws s3 cp s3://YOUR-BUCKET/get-pip.py .
pypy get-pip.py
sudo ln -sf /home/hadoop/pypy-5.4.1-linux_x86_64-portable.tar.bz2/bin/pypy /usr/bin/pip /usr/bin/pypy-pip
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you need to install more libraries use &lt;strong&gt;pypy-pip&lt;/strong&gt; instead of the normal &lt;strong&gt;pip&lt;/strong&gt;. This bootstrap also makes sure that the preinstalled python environment is left untouched as it can lead to problems if you do overwrite it.&lt;/p&gt;
&lt;h2&gt;Running hadoop jobs with pypy&lt;/h2&gt;
&lt;p&gt;If you bootstrapped your cluster correctly you should now be able to run hadoop streaming jobs with pypy. Refer to this &lt;a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_CreateStreaming.html"&gt;link&lt;/a&gt; on how to run a streaming step on emr. Obviously choose pypy instead of python2.7 for mapper and reducer excutable.&lt;/p&gt;
&lt;h2&gt;Run hadoop jobs with mrjob and pypy&lt;/h2&gt;
&lt;p&gt;Since i run most of my jobs with &lt;a href="http://pythonhosted.org/mrjob/"&gt;mrjob&lt;/a&gt; i needed to figure out on how to change the python binary. You can do so by specifing the &lt;a href="https://pythonhosted.org/mrjob/guides/configs-all-runners.html#option-python_bin"&gt;python_bin&lt;/a&gt; configuration.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raphael Brand</dc:creator><pubDate>Sun, 23 Oct 2016 20:23:00 +0200</pubDate><guid isPermaLink="false">tag:raphaelbrand.github.io,2016-10-23:run-hadoop-streaming-jobs-with-pypy-on-emr.html</guid><category>hadoop emr streaming python</category></item><item><title>Parallel processing with python</title><link>https://raphaelbrand.github.io/parallel-processing-with-python.html</link><description>&lt;p&gt;Since i do a lot of machine learning and nlp related tasks, i often need to preprocess large amounts of data. For web pages, which where obtained by a crawler, these steps might include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;remove html code and extract the raw text data&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"&gt;tokenize&lt;/a&gt; the raw text data&lt;/li&gt;
&lt;li&gt;filter unwanted tokens such as punctuation characters&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Stemming"&gt;stem&lt;/a&gt; the remaining tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a very simple example. Lets assume we have a text file where every line is a document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is an example text.
Another text with no meaning!
What is this? Some more text here.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The goal is to extract every word and write those tokens separated by whitespace to a new file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;

&lt;span class="n"&gt;tokenized_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tokenized_lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\W+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isalpha&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenized_lines&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It would obviously better not to accumulate all tokens in memory, but to write them to a file as soon as they are extracted. But for simplicity i will keep it this way. With enough data this step will take a long time. To reduce the execution time i tried to run the tasks in parallel, since every modern laptop has multiple cores and the tokenization task is cpu bound. &lt;/p&gt;
&lt;h4&gt;Requirements&lt;/h4&gt;
&lt;p&gt;Before i started to search for a solution i thought about the requirements it should have.&lt;/p&gt;
&lt;h5&gt;Overhead&lt;/h5&gt;
&lt;p&gt;I did not want to use one of the many existing &lt;a href="https://wiki.python.org/moin/ParallelProcessing"&gt;solutions&lt;/a&gt;. I rather searched for a solution based on the standard library with minimal overhead. I also did not need support for clustering, since i was restricted to one machine.&lt;/p&gt;
&lt;h5&gt;Multiprocessing&lt;/h5&gt;
&lt;p&gt;If you don't know about the GIL, you should read this &lt;a href="http://www.jeffknupp.com/blog/2012/03/31/pythons-hardest-problem/"&gt;blog post&lt;/a&gt;. Since there can be only one thread active at the time in python programs, the threading library is useless for cpu bound tasks. Instead one should use the &lt;a href="https://docs.python.org/2/library/multiprocessing.html"&gt;multiprocessing module&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;Streams&lt;/h5&gt;
&lt;p&gt;The data size was always too large to fit into memory. Because of that, i always used data streams, mostly contents of text files. The new solution should not only be able to support streams, but should also give the user the opportunity to regulate how much memory is used.&lt;/p&gt;
&lt;h4&gt;Solution&lt;/h4&gt;
&lt;p&gt;Based on my requirements i wrote a simple solution which is hosted on &lt;a href="https://github.com/raphaelbrand/PyProcessParallel"&gt;github&lt;/a&gt;. The architecture of the solution is best described in the following picture.&lt;/p&gt;
&lt;p&gt;&lt;img alt="task chain Text" src="https://raphaelbrand.github.io/images/PyProcessParallel.png" /&gt;&lt;/p&gt;
&lt;p&gt;The producer reads from a source and puts work into the work queue. It is possible to specify a size for each queue. If a queue is already at maximum capacity an attempt to put more items into will block the caller. This allows me to control the amount of memory which will be used. Every worker runs in its own process and takes work out of the work queue. After a worker has finished its task, it puts the result into the result queue. The consumer will then collect the results and write them to specified destination.&lt;/p&gt;
&lt;p&gt;Lets transform our example from the beginning to use the new script. I'll assume you already downloaded the script from github and included it in your project:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;process_parallel&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;process_parallel&lt;/span&gt;

&lt;span class="c1"&gt;# you could use the file object as the producer,&lt;/span&gt;
&lt;span class="c1"&gt;# no need to write a generator.&lt;/span&gt;
&lt;span class="c1"&gt;# But for demonstration purposes i&amp;#39;ll write a function&lt;/span&gt;
&lt;span class="c1"&gt;# to show you how to include other sources&lt;/span&gt;
&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;producer&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;worker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\W&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isalpha&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;consumer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# the producer should support __iter__&lt;/span&gt;
&lt;span class="c1"&gt;# worker and consumer need to be functions &lt;/span&gt;
&lt;span class="n"&gt;process_parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;producer&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;worker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer&lt;/span&gt;
                 &lt;span class="n"&gt;maxsize_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxsize_results&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#clean up&lt;/span&gt;
&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The solution is simple enough, only 50 lines of python code. The queues allow us to control the memory being used. On the downside there is a overhead due to the use of the processes. The data needs to be serialized and deserialized between processes. This means, if you're worker logic is too simple, then your code will run slower in parallel.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raphael Brand</dc:creator><pubDate>Tue, 02 Sep 2014 16:00:00 +0200</pubDate><guid isPermaLink="false">tag:raphaelbrand.github.io,2014-09-02:parallel-processing-with-python.html</guid><category>python parallel</category></item><item><title>How to host your blog on github with pelican.</title><link>https://raphaelbrand.github.io/how-to-host-your-blog-on-github-with-pelican.html</link><description>&lt;p&gt;&lt;a href="https://getpelican.com"&gt;Pelican&lt;/a&gt; is a static site generator written in python. You write your content in a markup language of your choice, in my case &lt;a href="http://en.wikipedia.org/wiki/Markdown"&gt;markdown&lt;/a&gt;, and the generator will transform it into static html sites. The html can then be uploaded to a hoster of your choosing. &lt;a href="https://github.com"&gt;Github&lt;/a&gt; offers the possibility to host user and projects pages. You can read about the difference &lt;a href="https://help.github.com/articles/user-organization-and-project-pages"&gt;here&lt;/a&gt;. In this tutorial i will assume that you host your blog as a user page.&lt;/p&gt;
&lt;p&gt;First of all  you will need to create a github repository with the following naming scheme: &lt;em&gt;username.github.io&lt;/em&gt; (replace &lt;em&gt;username&lt;/em&gt; with your github username). Then you can clone your repository with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone git@github.com:username/username.github.io.git
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next up we will generate a new virtual environment and install pelican. I will use a new branch for this, since i want to keep the source and actual blog separate. If you are not familiar with virtualenv or why you should use is it, you should read &lt;a href="http://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/"&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; username.github.io
git branch &lt;span class="nb"&gt;source&lt;/span&gt;
git checkout &lt;span class="nb"&gt;source&lt;/span&gt;
virtualenv env
&lt;span class="nb"&gt;source&lt;/span&gt; env/bin/activate
pip install pelican markdown ghp-import
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pelican comes with a script to bootstrap your blog. Run &lt;em&gt;pelican-quickstart&lt;/em&gt; and fill out the details. Make sure to generate a makefile, as it will be used later on. After you ran the command you can view and change your settings in pelicanconf.py. You can view my settings if you checkout the branch source of my blog repository. I also added the following to my .gitignore:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;env
output
*.pid
cache
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you can write your first post! Create a new file in the content folder and start typing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vim content/fist-post.md
Title: My first post!
Category: foo
Tags: foo bar

This is my first post
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make html
make serve
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and point your browser to &lt;a href="http://localhost:8000"&gt;localhost:8000&lt;/a&gt;. If everything looks good we can now deploy the blog to github. First push the source branch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout &lt;span class="nb"&gt;source&lt;/span&gt;
git add .
git commit -m &lt;span class="s2"&gt;&amp;quot;first post&amp;quot;&lt;/span&gt;
git push origin &lt;span class="nb"&gt;source&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then deploy to github with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make github
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Themes&lt;/h4&gt;
&lt;p&gt;Since i did not like the default theme, i chose another one. You can view a list of themes in the pelican-themes &lt;a href="https://github.com/getpelican/pelican-themes"&gt;repository&lt;/a&gt;. To install a theme just copy the theme folder to a location of your choice and edit the pelicanconfy.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;THEME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;path/to/your/theme&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Future work&lt;/h4&gt;
&lt;p&gt;You can heavily customize your blog. This is just a very simple solution to get you up and running. For example you can include comments with disqus or track your site with google analytics. For further reading i recommend the pelican &lt;a href="http://docs.getpelican.com/en/3.4.0/"&gt;documentation&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raphael Brand</dc:creator><pubDate>Mon, 01 Sep 2014 12:00:00 +0200</pubDate><guid isPermaLink="false">tag:raphaelbrand.github.io,2014-09-01:how-to-host-your-blog-on-github-with-pelican.html</guid><category>blog pelican python github</category></item></channel></rss>