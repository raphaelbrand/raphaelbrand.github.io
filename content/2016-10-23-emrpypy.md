Title: Run hadoop streaming jobs with pypy on EMR
Date: 2016-10-23 20:23
Category: big data
Tags: hadoop emr streaming python

While python is very nice language, it tends to be a lot slower for CPU bound tasks than compiled languages or languages with a [JIT](https://en.wikipedia.org/wiki/Just-in-time_compilation) compiler [\[1\]](http://benchmarksgame.alioth.debian.org/). While most people just use the standard CPython implementation, there are also alternatives out there. One of them being [pypy](http://pypy.org), which has a JIT compiler. The great thing about this is, that for some cases you get some big speedups without changing any thing in your code! Let's try it out with a simple example. We are going to load a file with 5 million lines where each line is a json object. The code looks like this:

```python
import fileinput
import json

if __name__ == '__main__':
    for line in fileinput.input():
        json.loads(line)
```
And the results are:

  * CPython: 50s
  * pypy: 9s

Yes you could also use a library like [ujson](https://pypi.python.org/pypi/ujson) to speed up json parsing, but this example is only to demonstrate how your code can run faster without having to change anything. But be aware that not all libraries are compatible with pypy. For more information read [here](http://pypy.org/compat.html). Also pypy usually uses more memory than CPython.

## Installing pypy on EMR
The easiest way to get pypy running on the EMR AMIs is to use the binaries by [squeaky-pl](https://github.com/squeaky-pl/portable-pypy). Since all your machines in the cluster have to pull this binary, it is best to put it in a s3 bucket first. Also put this [file](https://bootstrap.pypa.io/get-pip.py) in the bucket. Then add this to your bootstrap:

```bash
aws s3 cp s3://YOUR-BUCKET/pypy-5.4.1-linux_x86_64-portable.tar.bz2 .
mv pypy-5.4.1-linux_x86_64-portable.tar.bz2 /home/hadoop/
cd /home/hadoop/
tar -xjf pypy-5.4.1-linux_x86_64-portable.tar.bz2
sudo ln -sf /home/hadoop/pypy-5.4.1-linux_x86_64-portable.tar.bz2/bin/pypy /usr/bin/pypy
aws s3 cp s3://YOUR-BUCKET/get-pip.py .
pypy get-pip.py
sudo ln -sf /home/hadoop/pypy-5.4.1-linux_x86_64-portable.tar.bz2/bin/pypy /usr/bin/pip /usr/bin/pypy-pip
```

If you need to install more libraries use **pypy-pip** instead of the normal **pip**. This bootstrap also makes sure that the preinstalled python environment is left untouched as it can lead to problems if you do overwrite it.

## Running hadoop jobs with pypy
If you bootstrapped your cluster correctly you should now be able to run hadoop streaming jobs with pypy. Refer to this [link](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_CreateStreaming.html) on how to run a streaming step on emr. Obviously choose pypy instead of python2.7 for mapper and reducer excutable.

## Run hadoop jobs with mrjob and pypy
Since i run most of my jobs with [mrjob](http://pythonhosted.org/mrjob/) i needed to figure out on how to change the python binary. You can do so by specifing the [python_bin](https://pythonhosted.org/mrjob/guides/configs-all-runners.html#option-python_bin) configuration.
